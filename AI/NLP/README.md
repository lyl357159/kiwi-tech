# è‡ªç„¶è¯­è¨€å¤„ç†

## ğŸ“š å†…å®¹å¯¼èˆª

- [æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯](./TextPreprocessing.md) - åˆ†è¯ã€è¯å½¢è¿˜åŸã€åœç”¨è¯å¤„ç†ç­‰åŸºç¡€æ–‡æœ¬å¤„ç†æ–¹æ³•
- [è¯å‘é‡ä¸è¯­è¨€è¡¨ç¤º](./WordEmbeddings.md) - Word2Vecã€GloVeã€FastTextã€ä¸Šä¸‹æ–‡åµŒå…¥æ¨¡å‹
- [è¯­è¨€æ¨¡å‹æ¶æ„](./LanguageModels.md) - N-gramåˆ°Transformeræ¨¡å‹ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¶æ„è§£æ
- [åºåˆ—æ ‡æ³¨ä»»åŠ¡](./SequenceLabeling.md) - è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ç­‰æŠ€æœ¯
- [æ–‡æœ¬åˆ†ç±»ä¸æƒ…æ„Ÿåˆ†æ](./TextClassification.md) - æ–‡æ¡£åˆ†ç±»ã€æƒ…æ„Ÿææ€§åˆ†æä¸å¤šæ ‡ç­¾åˆ†ç±»æ–¹æ³•
- [æœºå™¨ç¿»è¯‘æŠ€æœ¯](./MachineTranslation.md) - ç»Ÿè®¡æœºå™¨ç¿»è¯‘ã€ç¥ç»æœºå™¨ç¿»è¯‘ä¸è¯„ä¼°æ–¹æ³•
- [é—®ç­”ç³»ç»Ÿ](./QuestionAnswering.md) - ä¿¡æ¯æ£€ç´¢å¼QAã€ç”Ÿæˆå¼QAä¸å¯¹è¯ç³»ç»Ÿè®¾è®¡

## ğŸ” è‡ªç„¶è¯­è¨€å¤„ç†çŸ¥è¯†ç»“æ„

```mermaid
graph TD
    A[è‡ªç„¶è¯­è¨€å¤„ç†] --> B[åŸºç¡€æŠ€æœ¯]
    A --> C[è¡¨ç¤ºå­¦ä¹ ]
    A --> D[æ ¸å¿ƒä»»åŠ¡]
    A --> E[åº”ç”¨ç³»ç»Ÿ]
    A --> F[è¯„ä¼°æ–¹æ³•]
    
    B --> B1[æ–‡æœ¬é¢„å¤„ç†]
    B --> B2[è¯­è¨€å­¦åŸºç¡€]
    B --> B3[ç»Ÿè®¡è¯­è¨€æ¨¡å‹]
    
    C --> C1[è¯è¡¨ç¤º]
    C --> C2[å¥å­è¡¨ç¤º]
    C --> C3[æ–‡æ¡£è¡¨ç¤º]
    C --> C4[è·¨è¯­è¨€è¡¨ç¤º]
    
    D --> D1[è¯æ³•åˆ†æ]
    D --> D2[å¥æ³•åˆ†æ]
    D --> D3[è¯­ä¹‰åˆ†æ]
    D --> D4[ç¯‡ç« åˆ†æ]
    
    E --> E1[ä¿¡æ¯æŠ½å–]
    E --> E2[æ–‡æœ¬ç”Ÿæˆ]
    E --> E3[å¯¹è¯ç³»ç»Ÿ]
    E --> E4[æœºå™¨ç¿»è¯‘]
    E --> E5[å¤šæ¨¡æ€NLP]
    
    F --> F1[å†…åœ¨è¯„ä¼°]
    F --> F2[å¤–åœ¨è¯„ä¼°]
    F --> F3[äººå·¥è¯„ä¼°]
    
    B1 --> B11[åˆ†è¯ä¸æ ‡è®°åŒ–]
    B1 --> B12[è¯å½¢è¿˜åŸ]
    B1 --> B13[æ•°æ®å¢å¼º]
    
    C1 --> C11[One-hotç¼–ç ]
    C1 --> C12[åˆ†å¸ƒå¼è¡¨ç¤º]
    C1 --> C13[ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤º]
    
    D1 --> D11[è¯æ€§æ ‡æ³¨]
    D1 --> D12[å‘½åå®ä½“è¯†åˆ«]
    D1 --> D13[è¯ä¹‰æ¶ˆæ­§]
    
    D2 --> D21[ä¾å­˜å¥æ³•åˆ†æ]
    D2 --> D22[æˆåˆ†å¥æ³•åˆ†æ]
    
    D3 --> D31[è¯­ä¹‰è§’è‰²æ ‡æ³¨]
    D3 --> D32[è¯­ä¹‰å…³ç³»æå–]
    
    E1 --> E11[å®ä½“æŠ½å–]
    E1 --> E12[å…³ç³»æŠ½å–]
    E1 --> E13[äº‹ä»¶æŠ½å–]
    
    E3 --> E31[ä»»åŠ¡å‹å¯¹è¯]
    E3 --> E32[å¼€æ”¾åŸŸå¯¹è¯]
    E3 --> E33[æƒ…æ„Ÿå¯¹è¯]
    
    classDef basic fill:#f9d5e5,stroke:#333,stroke-width:1px;
    classDef rep fill:#eeac99,stroke:#333,stroke-width:1px;
    classDef task fill:#e06377,stroke:#333,stroke-width:1px;
    classDef app fill:#c83349,stroke:#333,stroke-width:1px;
    classDef eval fill:#5b9aa0,stroke:#333,stroke-width:1px;
    
    class B1,B2,B3,B11,B12,B13 basic;
    class C1,C2,C3,C4,C11,C12,C13 rep;
    class D1,D2,D3,D4,D11,D12,D13,D21,D22,D31,D32 task;
    class E1,E2,E3,E4,E5,E11,E12,E13,E31,E32,E33 app;
    class F1,F2,F3 eval;
```

## ğŸ“Š NLPæŠ€æœ¯å‘å±•ä¸å¯¹æ¯”

| æŠ€æœ¯é¢†åŸŸ | ä¼ ç»Ÿæ–¹æ³• | æ·±åº¦å­¦ä¹ æ–¹æ³• | é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ | å½“å‰è¶‹åŠ¿ |
|---------|---------|------------|--------------|---------|
| æ–‡æœ¬è¡¨ç¤º | TF-IDF, BoW | Word2Vec, GloVe | BERT, RoBERTa | è‡ªé€‚åº”è¡¨ç¤ºã€å¤šè¯­è¨€æ¨¡å‹ |
| æ–‡æœ¬åˆ†ç±» | SVM, Naive Bayes | CNN, RNN, LSTM | å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ | å°‘æ ·æœ¬å­¦ä¹ ã€æ•°æ®å¢å¼º |
| åºåˆ—æ ‡æ³¨ | HMM, CRF | BiLSTM-CRF | å¾®è°ƒBERT/spané¢„æµ‹ | ç»“æ„åŒ–é¢„æµ‹ã€å¤šä»»åŠ¡å­¦ä¹  |
| å¥æ³•åˆ†æ | åŸºäºè§„åˆ™, PCFG | ç¥ç»å¥æ³•åˆ†æå™¨ | åŸºäºå›¾çš„è¯­æ³•åˆ†æ | æ— ç›‘ç£ã€è·¨è¯­è¨€ |
| æœºå™¨ç¿»è¯‘ | åŸºäºè§„åˆ™/ç»Ÿè®¡ | Seq2Seq, æ³¨æ„åŠ› | Transformer | å¤šè¯­è¨€ã€æ–‡æ¡£çº§ç¿»è¯‘ |
| å¯¹è¯ç³»ç»Ÿ | åŸºäºæ¨¡æ¿/çŠ¶æ€ | æ£€ç´¢å¼, ç”Ÿæˆå¼ | å¤§è§„æ¨¡é¢„è®­ç»ƒ | å¤šè½®æ¨ç†ã€çŸ¥è¯†æ•´åˆ |
| æ–‡æœ¬ç”Ÿæˆ | æ¨¡æ¿å¡«å…… | RNN/Transformer | GPTç³»åˆ— | å¯æ§ç”Ÿæˆã€é•¿æ–‡æœ¬ç”Ÿæˆ |

## ğŸš€ NLPå¤„ç†æµç¨‹

```mermaid
flowchart TD
    A[åŸå§‹æ–‡æœ¬] --> B[é¢„å¤„ç†]
    B --> C[ç‰¹å¾æå–/è¡¨ç¤ºå­¦ä¹ ]
    C --> D[æ¨¡å‹è®­ç»ƒ]
    D --> E[è¯„ä¼°ä¸ä¼˜åŒ–]
    E --> F{æ»¡è¶³è¦æ±‚?}
    F -- å¦ --> |è°ƒæ•´å‚æ•°/æ¨¡å‹| D
    F -- å¦ --> |æ”¹è¿›ç‰¹å¾| C
    F -- æ˜¯ --> G[éƒ¨ç½²åº”ç”¨]
    
    subgraph æ•°æ®å‡†å¤‡é˜¶æ®µ
    A
    B
    end
    
    subgraph ç‰¹å¾å·¥ç¨‹ä¸å»ºæ¨¡é˜¶æ®µ
    C
    D
    E
    F
    end
    
    subgraph åº”ç”¨éƒ¨ç½²é˜¶æ®µ
    G
    end
    
    B --> B1[åˆ†è¯/æ ‡è®°åŒ–]
    B1 --> B2[å»é™¤åœç”¨è¯]
    B2 --> B3[è¯å½¢è¿˜åŸ/è¯å¹²æå–]
    B3 --> B4[æ•°æ®å¢å¼º/æ¸…æ´—]
```

## ğŸ“ NLPè¯„ä¼°æŒ‡æ ‡

### æ–‡æœ¬åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è®¡ç®—æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|------|---------|---------|-------|
| å‡†ç¡®ç‡(Accuracy) | æ­£ç¡®åˆ†ç±»æ ·æœ¬æ•°/æ€»æ ·æœ¬æ•° | ç±»åˆ«å‡è¡¡é—®é¢˜ | ç›´è§‚ä½†åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šæœ‰åå·® |
| ç²¾ç¡®ç‡(Precision) | TP/(TP+FP) | å‡å°‘è¯¯æŠ¥é‡è¦åœºæ™¯ | è¡¡é‡æ­£ä¾‹é¢„æµ‹çš„å‡†ç¡®æ€§ |
| å¬å›ç‡(Recall) | TP/(TP+FN) | å‡å°‘æ¼æŠ¥é‡è¦åœºæ™¯ | è¡¡é‡æ•è·æ‰€æœ‰æ­£ä¾‹çš„èƒ½åŠ› |
| F1åˆ†æ•° | 2Ã—(PÃ—R)/(P+R) | ç²¾ç¡®ä¸å¬å›å¹³è¡¡ | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ |
| Macro-F1 | å„ç±»F1çš„å¹³å‡ | å„ç±»åŒç­‰é‡è¦ | åæ˜ æ‰€æœ‰ç±»åˆ«çš„æ•´ä½“è¡¨ç° |
| Micro-F1 | æ±‡æ€»å„ç±»ç»“æœè®¡ç®— | è€ƒè™‘æ ·æœ¬åˆ†å¸ƒ | å—ä¸»ç±»å½±å“è¾ƒå¤§ |

### åºåˆ—æ ‡æ³¨è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è®¡ç®—æ–¹æ³• | é€‚ç”¨åœºæ™¯ | å¤‡æ³¨ |
|------|---------|---------|-----|
| æ ‡è®°å‡†ç¡®ç‡ | æ­£ç¡®æ ‡æ³¨æ ‡è®°æ•°/æ€»æ ‡è®°æ•° | ä¸€èˆ¬åºåˆ—æ ‡æ³¨ | ç®€å•ä½†ä¸åæ˜ å®ä½“å®Œæ•´æ€§ |
| å®ä½“ç²¾ç¡®ç‡ | æ­£ç¡®æ ‡æ³¨å®ä½“æ•°/é¢„æµ‹å®ä½“æ€»æ•° | NERä»»åŠ¡ | è¡¡é‡å®ä½“è¾¹ç•Œè¯†åˆ«å‡†ç¡®æ€§ |
| å®ä½“å¬å›ç‡ | æ­£ç¡®æ ‡æ³¨å®ä½“æ•°/å®é™…å®ä½“æ€»æ•° | NERä»»åŠ¡ | è¡¡é‡å‘ç°æ‰€æœ‰å®ä½“çš„èƒ½åŠ› |
| å®ä½“F1 | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ | NERä»»åŠ¡ | ç»¼åˆè¯„ä»·å®ä½“æŠ½å–æ€§èƒ½ |

### æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è®¡ç®—åŸç† | ä¼˜ç‚¹ | å±€é™æ€§ |
|------|---------|-----|--------|
| BLEU | n-gramç²¾ç¡®åŒ¹é… | å¹¿æ³›ä½¿ç”¨ï¼Œè®¡ç®—ç®€å• | ä¸è€ƒè™‘è¯­ä¹‰ç­‰ä»·æ€§ |
| METEOR | åŸºäºå¯¹é½çš„å•è¯åŒ¹é… | è€ƒè™‘åŒä¹‰è¯å’Œè¯å½¢å˜åŒ– | è®¡ç®—å¤æ‚åº¦é«˜ |
| chrF | å­—ç¬¦çº§åˆ«çš„F-score | é€‚åˆå½¢æ€ä¸°å¯Œçš„è¯­è¨€ | å¯¹çŸ­æ–‡æœ¬æ•ˆæœæœ‰é™ |
| COMET | åŸºäºç¥ç»ç½‘ç»œçš„è¯„ä¼° | ä¸äººç±»åˆ¤æ–­æ›´ç›¸å…³ | éœ€è¦è®­ç»ƒæ¨¡å‹ |
| BERTScore | ä¸Šä¸‹æ–‡åµŒå…¥ç›¸ä¼¼åº¦ | è€ƒè™‘è¯­ä¹‰ç›¸ä¼¼æ€§ | è®¡ç®—èµ„æºæ¶ˆè€—å¤§ |

## ğŸ’¡ NLPæœ€ä½³å®è·µ

### æ–‡æœ¬é¢„å¤„ç†æŠ€å·§

```python
# æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹ä»£ç 
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

nltk.download(['punkt', 'wordnet', 'stopwords'])

def preprocess_text(text, language='english'):
    """åŸºæœ¬çš„æ–‡æœ¬é¢„å¤„ç†å‡½æ•°"""
    # è½¬å°å†™
    text = text.lower()
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # åˆ†è¯
    tokens = word_tokenize(text)
    
    # ç§»é™¤åœç”¨è¯
    stop_words = set(stopwords.words(language))
    tokens = [t for t in tokens if t not in stop_words]
    
    # è¯å½¢è¿˜åŸ
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    
    return tokens

# ä½¿ç”¨ç¤ºä¾‹
text = "The quick brown foxes jumped over the lazy dogs! They weren't very quick."
processed_tokens = preprocess_text(text)
print(processed_tokens)
```

### æ„å»ºæ–‡æœ¬åˆ†ç±»æµæ°´çº¿

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# ç¤ºä¾‹æ–‡æœ¬æ•°æ®å’Œæ ‡ç­¾
texts = ["This movie is great!", "I hate this product", "Love the service", ...]
labels = [1, 0, 1, ...]  # 1ä¸ºæ­£é¢æƒ…æ„Ÿï¼Œ0ä¸ºè´Ÿé¢æƒ…æ„Ÿ

# åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)

# åˆ›å»ºå¤„ç†æµæ°´çº¿
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
    ('clf', LogisticRegression(C=1, max_iter=1000))
])

# å‚æ•°ç½‘æ ¼æœç´¢
param_grid = {
    'tfidf__max_features': [3000, 5000, 10000],
    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'clf__C': [0.1, 1.0, 10.0]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')
grid_search.fit(X_train, y_train)

# æœ€ä½³å‚æ•°
print("æœ€ä½³å‚æ•°:", grid_search.best_params_)

# è¯„ä¼°æ¨¡å‹
y_pred = grid_search.predict(X_test)
print(classification_report(y_test, y_pred))
```

### BERTå¾®è°ƒæœ€ä½³å®è·µ

```python
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 2. æ•°æ®å‡†å¤‡
def encode_text(texts, max_length=128):
    return tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

# å‡è®¾æˆ‘ä»¬å·²æœ‰è®­ç»ƒæ•°æ®
train_encodings = encode_text(train_texts)
val_encodings = encode_text(val_texts)

train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(train_labels)
)
val_dataset = TensorDataset(
    val_encodings['input_ids'],
    val_encodings['attention_mask'],
    torch.tensor(val_labels)
)

# 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# 4. è®­ç»ƒè®¾ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
total_steps = len(train_loader) * 3  # 3ä¸ªepoch
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# 5. è®­ç»ƒå¾ªç¯
best_f1 = 0.0

for epoch in range(3):
    # è®­ç»ƒæ¨¡å¼
    model.train()
    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
    
    # è¯„ä¼°æ¨¡å¼
    model.eval()
    val_preds = []
    val_true = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            outputs = model(input_ids, attention_mask=attention_mask)
            
            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()
            val_preds.extend(preds)
            val_true.extend(labels.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(val_true, val_preds)
    f1 = f1_score(val_true, val_preds)
    print(f"Epoch {epoch+1}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}")
    
    if f1 > best_f1:
        best_f1 = f1
        torch.save(model.state_dict(), 'best_model.pt')
```

## ğŸ“˜ ç›¸å…³èµ„æº

- [è¿”å›AIæŠ€æœ¯çŸ¥è¯†åº“é¦–é¡µ](../README.md)
- [æœºå™¨å­¦ä¹ åŸºç¡€](../MachineLearning/README.md)
- [æ·±åº¦å­¦ä¹ ](../DeepLearning/README.md)
- [å¤§æ¨¡å‹æŠ€æœ¯](../LargeModels/README.md)

### æ¨èå­¦ä¹ èµ„æº

- ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ç»¼è®ºã€‹(Daniel Jurafsky, James H. Martin)
- ã€ŠPythonè‡ªç„¶è¯­è¨€å¤„ç†ã€‹(Steven Bird, Ewan Klein, Edward Loper)
- [Stanford CS224n: æ·±åº¦å­¦ä¹ ä¸è‡ªç„¶è¯­è¨€å¤„ç†](http://web.stanford.edu/class/cs224n/)
- [Hugging Face Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers/index)
- [spaCyè‡ªç„¶è¯­è¨€å¤„ç†åº“](https://spacy.io/usage/spacy-101)
- [NLTKæ–‡æ¡£](https://www.nltk.org/)

---

Â© AIæŠ€æœ¯çŸ¥è¯†åº“ 2023 