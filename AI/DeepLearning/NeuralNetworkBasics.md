# ç¥ç»ç½‘ç»œåŸºç¡€

## ğŸ“š ç›®å½•

1. [ç¥ç»å…ƒæ¨¡å‹](#ç¥ç»å…ƒæ¨¡å‹)
2. [æ¿€æ´»å‡½æ•°](#æ¿€æ´»å‡½æ•°)
3. [ç¥ç»ç½‘ç»œæ¶æ„](#ç¥ç»ç½‘ç»œæ¶æ„)
4. [å‰å‘ä¼ æ’­](#å‰å‘ä¼ æ’­)
5. [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
6. [åå‘ä¼ æ’­ç®—æ³•](#åå‘ä¼ æ’­ç®—æ³•)
7. [å‚æ•°åˆå§‹åŒ–](#å‚æ•°åˆå§‹åŒ–)
8. [æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸](#æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸)
9. [ç¥ç»ç½‘ç»œå®ç°ç¤ºä¾‹](#ç¥ç»ç½‘ç»œå®ç°ç¤ºä¾‹)

## ç¥ç»å…ƒæ¨¡å‹

### ç”Ÿç‰©ç¥ç»å…ƒä¸äººå·¥ç¥ç»å…ƒå¯¹æ¯”

```mermaid
graph LR
    subgraph ç”Ÿç‰©ç¥ç»å…ƒ
    A1[æ ‘çª] --> B1[ç»†èƒä½“]
    B1 --> C1[è½´çª]
    C1 --> D1[çªè§¦]
    end
    
    subgraph äººå·¥ç¥ç»å…ƒ
    A2[è¾“å…¥ x] --> B2[åŠ æƒæ±‚å’Œ Î£]
    B2 --> C2[æ¿€æ´»å‡½æ•° f]
    C2 --> D2[è¾“å‡º y]
    E2[æƒé‡ w] --> B2
    F2[åç½® b] --> B2
    end
```

### äººå·¥ç¥ç»å…ƒæ•°å­¦è¡¨è¾¾

äººå·¥ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œå…¶æ•°å­¦è¡¨è¾¾ä¸ºï¼š

$$y = f(\sum_{i=1}^{n} w_i x_i + b)$$

å…¶ä¸­ï¼š
- $x_i$ æ˜¯è¾“å…¥ç‰¹å¾
- $w_i$ æ˜¯å¯¹åº”çš„æƒé‡
- $b$ æ˜¯åç½®é¡¹
- $f$ æ˜¯æ¿€æ´»å‡½æ•°
- $y$ æ˜¯ç¥ç»å…ƒçš„è¾“å‡º

**å‘é‡å½¢å¼**:
$$y = f(w^T x + b)$$

## æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°èµ‹äºˆç¥ç»ç½‘ç»œéçº¿æ€§å»ºæ¨¡èƒ½åŠ›ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„å…³é”®ç»„ä»¶ã€‚

### å¸¸ç”¨æ¿€æ´»å‡½æ•°å¯¹æ¯”

| æ¿€æ´»å‡½æ•° | æ•°å­¦è¡¨è¾¾å¼ | å€¼åŸŸ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|----------|------|-----|------|---------|
| Sigmoid | $\sigma(x) = \frac{1}{1+e^{-x}}$ | (0, 1) | å¹³æ»‘ã€å¯å¾®ã€è¾“å‡ºå¯è§£é‡Šä¸ºæ¦‚ç‡ | æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€è®¡ç®—å¼€é”€å¤§ã€è¾“å‡ºä¸æ˜¯é›¶ä¸­å¿ƒ | äºŒåˆ†ç±»è¾“å‡ºå±‚ |
| Tanh | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | é›¶ä¸­å¿ƒåŒ–ã€å¹³æ»‘å¯å¾® | æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€è®¡ç®—å¼€é”€å¤§ | æ—©æœŸRNNã€éšè—å±‚ |
| ReLU | $f(x) = \max(0, x)$ | [0, +âˆ) | è®¡ç®—é«˜æ•ˆã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€ç¨€ç–æ¿€æ´» | æ­»äº¡ReLUé—®é¢˜ã€éé›¶ä¸­å¿ƒã€éæœ‰ç•Œ | CNNéšè—å±‚ã€å¤§å¤šæ•°å‰é¦ˆç½‘ç»œ |
| Leaky ReLU | $f(x) = \max(\alpha x, x)$ | (-âˆ, +âˆ) | è§£å†³æ­»äº¡ReLUé—®é¢˜ã€ä¿ç•™è´Ÿæ¢¯åº¦ä¿¡æ¯ | éœ€è¦é¢å¤–è¶…å‚æ•°Î± | ReLUæ›¿ä»£æ–¹æ¡ˆ |
| ELU | $f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x-1) & \text{if } x \leq 0 \end{cases}$ | (-Î±, +âˆ) | è¾“å‡ºå‡å€¼æ¥è¿‘é›¶ã€å¹³æ»‘ | è®¡ç®—å¼€é”€ç›¸å¯¹å¤§ | å½“éœ€è¦è´Ÿå€¼æœ‰æ„ä¹‰æ—¶ |
| GELU | $f(x) = x \cdot \Phi(x)$ | (-âˆ, +âˆ) | æ€§èƒ½ä¼˜å¼‚ã€å¹³æ»‘ | è®¡ç®—å¤æ‚ | Transformerã€æœ€æ–°æ¨¡å‹ |
| Swish | $f(x) = x \cdot \sigma(\beta x)$ | (-âˆ, +âˆ) | å¹³æ»‘ã€è‡ªé—¨æ§ | è®¡ç®—å¤æ‚ | æ·±åº¦ç½‘ç»œ |

### æ¿€æ´»å‡½æ•°å¯è§†åŒ–æ¯”è¾ƒ

```
Sigmoid:    ------/â€¾â€¾â€¾â€¾â€¾â€¾â€¾
Tanh:       ---/â€¾â€¾â€¾\___
ReLU:       __|/â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
Leaky ReLU: _/|/â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
ELU:        _-|/â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
```

```python
# æ¿€æ´»å‡½æ•°å®ç°
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)

def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

def swish(x, beta=1.0):
    return x * sigmoid(beta * x)

# å¯è§†åŒ–æ¯”è¾ƒ
x = np.linspace(-5, 5, 1000)
plt.figure(figsize=(12, 8))
plt.plot(x, sigmoid(x), label='Sigmoid')
plt.plot(x, tanh(x), label='Tanh')
plt.plot(x, relu(x), label='ReLU')
plt.plot(x, leaky_relu(x), label='Leaky ReLU')
plt.plot(x, elu(x), label='ELU')
plt.plot(x, gelu(x), label='GELU')
plt.plot(x, swish(x), label='Swish')
plt.grid(True)
plt.legend()
plt.title('æ¿€æ´»å‡½æ•°æ¯”è¾ƒ')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
```

## ç¥ç»ç½‘ç»œæ¶æ„

### å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰

æœ€åŸºæœ¬çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¿¡æ¯åªä»è¾“å…¥å±‚å‘è¾“å‡ºå±‚å•å‘ä¼ æ’­ã€‚

```mermaid
graph LR
    subgraph è¾“å…¥å±‚
    I1[X1]
    I2[X2]
    I3[X3]
    end
    
    subgraph éšè—å±‚1
    H11[H11]
    H12[H12]
    H13[H13]
    H14[H14]
    end
    
    subgraph éšè—å±‚2
    H21[H21]
    H22[H22]
    H23[H23]
    end
    
    subgraph è¾“å‡ºå±‚
    O1[Y1]
    O2[Y2]
    end
    
    I1 --> H11
    I1 --> H12
    I1 --> H13
    I1 --> H14
    I2 --> H11
    I2 --> H12
    I2 --> H13
    I2 --> H14
    I3 --> H11
    I3 --> H12
    I3 --> H13
    I3 --> H14
    
    H11 --> H21
    H11 --> H22
    H11 --> H23
    H12 --> H21
    H12 --> H22
    H12 --> H23
    H13 --> H21
    H13 --> H22
    H13 --> H23
    H14 --> H21
    H14 --> H22
    H14 --> H23
    
    H21 --> O1
    H21 --> O2
    H22 --> O1
    H22 --> O2
    H23 --> O1
    H23 --> O2
```

**ä¸»è¦ç‰¹ç‚¹**:
- å±‚é—´å…¨è¿æ¥
- æ— ç¯ç»“æ„
- éšè—å±‚ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°
- å¯ä»¥æ‹Ÿåˆä»»æ„è¿ç»­å‡½æ•°ï¼ˆé€šç”¨è¿‘ä¼¼å®šç†ï¼‰

**æ•°å­¦è¡¨è¾¾**:
- ç¬¬ä¸€éšè—å±‚: $H^{(1)} = f_1(W^{(1)}X + b^{(1)})$
- ç¬¬äºŒéšè—å±‚: $H^{(2)} = f_2(W^{(2)}H^{(1)} + b^{(2)})$
- è¾“å‡ºå±‚: $Y = f_3(W^{(3)}H^{(2)} + b^{(3)})$

å…¶ä¸­ï¼Œ$W^{(l)}$æ˜¯ç¬¬$l$å±‚çš„æƒé‡çŸ©é˜µï¼Œ$b^{(l)}$æ˜¯åç½®å‘é‡ï¼Œ$f_l$æ˜¯æ¿€æ´»å‡½æ•°ã€‚

## å‰å‘ä¼ æ’­

å‰å‘ä¼ æ’­æ˜¯ä¿¡æ¯ä»è¾“å…¥å±‚é€šè¿‡ç½‘ç»œå‘è¾“å‡ºå±‚æµåŠ¨çš„è¿‡ç¨‹ã€‚

### å‰å‘ä¼ æ’­ç®—æ³•

```
è¾“å…¥: æ ·æœ¬x, ç½‘ç»œå‚æ•°{W^(l), b^(l)}
è¾“å‡º: ç½‘ç»œé¢„æµ‹å€¼y_pred

ç®—æ³•:
1. a^(0) = x  # åˆå§‹æ¿€æ´»å€¼ä¸ºè¾“å…¥
2. å¯¹äºæ¯ä¸€å±‚l=1åˆ°L:
   a. z^(l) = W^(l)a^(l-1) + b^(l)  # è®¡ç®—åŠ æƒå’Œ
   b. a^(l) = f^(l)(z^(l))         # åº”ç”¨æ¿€æ´»å‡½æ•°
3. y_pred = a^(L)  # è¾“å‡ºå±‚çš„æ¿€æ´»å€¼å³ä¸ºé¢„æµ‹å€¼
```

### å‰å‘ä¼ æ’­ç¤ºä¾‹

ä»¥ä¸¤å±‚ç½‘ç»œä¸ºä¾‹ï¼ˆ1ä¸ªéšè—å±‚ï¼Œ1ä¸ªè¾“å‡ºå±‚ï¼‰ï¼š

```python
def forward_pass(x, W1, b1, W2, b2, activation_fn):
    # ç¬¬ä¸€å±‚
    z1 = np.dot(W1, x) + b1
    a1 = activation_fn(z1)
    
    # ç¬¬äºŒå±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
    z2 = np.dot(W2, a1) + b2
    a2 = z2  # å‡è®¾è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°ï¼ˆå›å½’é—®é¢˜ï¼‰
    
    # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
    cache = (x, z1, a1, W1, b1, W2, b2)
    
    return a2, cache
```

## æŸå¤±å‡½æ•°

æŸå¤±å‡½æ•°é‡åŒ–æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®è·ï¼Œæ˜¯ç¥ç»ç½‘ç»œä¼˜åŒ–çš„ç›®æ ‡ã€‚

### å¸¸ç”¨æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•° | å…¬å¼ | é€‚ç”¨ä»»åŠ¡ | ç‰¹ç‚¹ |
|---------|-----|---------|------|
| å‡æ–¹è¯¯å·®(MSE) | $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ | å›å½’ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ |
| å¹³å‡ç»å¯¹è¯¯å·®(MAE) | $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$ | å›å½’ | å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼Œä½†ä¸å¯å¾® |
| äº¤å‰ç†µæŸå¤± | $-\sum_{i=1}^{C}y_i\log(\hat{y}_i)$ | åˆ†ç±» | æƒ©ç½šé”™è¯¯é¢„æµ‹çš„æ¦‚ç‡ |
| äºŒå…ƒäº¤å‰ç†µ | $-[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$ | äºŒåˆ†ç±» | æ•°å€¼ç¨³å®šæ€§å¥½ |
| Hinge Loss | $\max(0, 1 - y \cdot \hat{y})$ | SVMåˆ†ç±» | æœ€å¤§é—´éš”åˆ†ç±» |
| Huber Loss | ç»“åˆMSEå’ŒMAEçš„ç‰¹ç‚¹ | å›å½’ | å¯¹å¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§ |

## åå‘ä¼ æ’­ç®—æ³•

åå‘ä¼ æ’­æ˜¯è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦çš„é«˜æ•ˆç®—æ³•ï¼Œæ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒã€‚

### åŸºæœ¬åŸç†

åå‘ä¼ æ’­åˆ©ç”¨é“¾å¼æ³•åˆ™é€å±‚è®¡ç®—æ¢¯åº¦ï¼Œä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­è¯¯å·®ä¿¡å·ã€‚

```mermaid
graph RL
    A[è®¡ç®—æŸå¤±å‡½æ•°] --> B[è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦]
    B --> C[è®¡ç®—éšè—å±‚æ¢¯åº¦]
    C --> D[è®¡ç®—è¾“å…¥å±‚æ¢¯åº¦]
    D --> E[æ›´æ–°å‚æ•°]
```

### æ•°å­¦æ¨å¯¼

å¯¹äºæŸå¤±å‡½æ•°$J$å’Œå‚æ•°$\theta$ï¼Œæ¢¯åº¦$\frac{\partial J}{\partial \theta}$çš„è®¡ç®—å¦‚ä¸‹ï¼š

1. **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—æ¯å±‚çš„åŠ æƒå’Œ$z^{(l)}$å’Œæ¿€æ´»å€¼$a^{(l)}$

2. **è¾“å‡ºå±‚è¯¯å·®**ï¼šè®¡ç®—è¾“å‡ºå±‚è¯¯å·®$\delta^{(L)} = \frac{\partial J}{\partial z^{(L)}}$
   - å¯¹äºMSEæŸå¤±ï¼š$\delta^{(L)} = (a^{(L)} - y) \odot f'(z^{(L)})$
   - å¯¹äºäº¤å‰ç†µæŸå¤±é…åˆSoftmaxæ¿€æ´»ï¼š$\delta^{(L)} = a^{(L)} - y$

3. **åå‘ä¼ æ’­è¯¯å·®**ï¼šé€å±‚åå‘è®¡ç®—è¯¯å·®
   $\delta^{(l)} = ((W^{(l+1)})^T \delta^{(l+1)}) \odot f'(z^{(l)})$

4. **è®¡ç®—æ¢¯åº¦**ï¼š
   - æƒé‡æ¢¯åº¦ï¼š$\frac{\partial J}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$
   - åç½®æ¢¯åº¦ï¼š$\frac{\partial J}{\partial b^{(l)}} = \delta^{(l)}$

### åå‘ä¼ æ’­ç¤ºä¾‹

```python
def backward_pass(y_true, y_pred, cache, activation_fn_derivative):
    x, z1, a1, W1, b1, W2, b2 = cache
    
    # è¾“å‡ºå±‚è¯¯å·®
    dz2 = y_pred - y_true  # å‡è®¾MSEæŸå¤±
    
    # è¾“å‡ºå±‚å‚æ•°æ¢¯åº¦
    dW2 = np.dot(dz2, a1.T)
    db2 = dz2
    
    # éšè—å±‚è¯¯å·®
    dz1 = np.dot(W2.T, dz2) * activation_fn_derivative(z1)
    
    # éšè—å±‚å‚æ•°æ¢¯åº¦
    dW1 = np.dot(dz1, x.T)
    db1 = dz1
    
    return dW1, db1, dW2, db2
```

## å‚æ•°åˆå§‹åŒ–

ç¥ç»ç½‘ç»œå‚æ•°åˆå§‹åŒ–å¯¹è®­ç»ƒè¿‡ç¨‹å’Œæœ€ç»ˆæ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚

### å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•

| åˆå§‹åŒ–æ–¹æ³• | å…¬å¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----------|-----|------|---------|
| é›¶åˆå§‹åŒ– | $W = 0$ | å¯¹ç§°æ€§é—®é¢˜ï¼Œä¸é€‚ç”¨äºæ·±åº¦ç½‘ç»œ | ä»…ç”¨äºåç½®é¡¹ |
| éšæœºåˆå§‹åŒ– | $W \sim U(-r, r)$ | æ‰“ç ´å¯¹ç§°æ€§ | æµ…å±‚ç½‘ç»œ |
| Xavier/Glorot | $W \sim U(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}})$ | ä¿æŒå„å±‚æ–¹å·®ä¸€è‡´ | Sigmoid/Tanhæ¿€æ´»å‡½æ•° |
| Heåˆå§‹åŒ– | $W \sim N(0, \sqrt{\frac{2}{n_{in}}})$ | é€‚åˆReLUæ¿€æ´»å‡½æ•° | ReLUåŠå…¶å˜ç§ |
| æ­£äº¤åˆå§‹åŒ– | ä½¿æƒé‡çŸ©é˜µæ¥è¿‘æ­£äº¤çŸ©é˜µ | æ”¹å–„ä¿¡å·ä¼ æ’­ | æ·±åº¦ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯RNN |

```python
# å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•å®ç°
def zero_initialization(shape):
    return np.zeros(shape)

def random_initialization(shape, scale=0.01):
    return np.random.randn(*shape) * scale

def xavier_initialization(shape):
    n_in, n_out = shape
    limit = np.sqrt(6 / (n_in + n_out))
    return np.random.uniform(-limit, limit, shape)

def he_initialization(shape):
    n_in, n_out = shape
    std = np.sqrt(2 / n_in)
    return np.random.randn(n_in, n_out) * std
```

## æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸

åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¢¯åº¦å¯èƒ½éšç€å±‚æ•°çš„å¢åŠ è€Œå˜å¾—æå°ï¼ˆæ¶ˆå¤±ï¼‰æˆ–æå¤§ï¼ˆçˆ†ç‚¸ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ã€‚

### æ¢¯åº¦æ¶ˆå¤±

**åŸå› **:
- Sigmoid/Tanhæ¿€æ´»å‡½æ•°çš„å¯¼æ•°åœ¨é¥±å’ŒåŒºåŸŸæ¥è¿‘é›¶
- æ·±å±‚ç½‘ç»œä¸­çš„è¿ä¹˜æ•ˆåº”

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ReLUåŠå…¶å˜ç§ä½œä¸ºæ¿€æ´»å‡½æ•°
2. æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰
3. æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰
4. åˆé€‚çš„æƒé‡åˆå§‹åŒ–

### æ¢¯åº¦çˆ†ç‚¸

**åŸå› **:
- æƒé‡å€¼è¿‡å¤§
- å­¦ä¹ ç‡è®¾ç½®ä¸å½“
- æ·±å±‚ç½‘ç»œä¸­çš„è¿ä¹˜æ•ˆåº”

**è§£å†³æ–¹æ¡ˆ**:
1. æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰
2. æƒé‡æ­£åˆ™åŒ–
3. æ‰¹å½’ä¸€åŒ–
4. åˆé€‚çš„æƒé‡åˆå§‹åŒ–

## ç¥ç»ç½‘ç»œå®ç°ç¤ºä¾‹

å®Œæ•´å®ç°ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œï¼š

```python
import numpy as np

class SimpleNN:
    def __init__(self, input_size, hidden_size, output_size):
        # åˆå§‹åŒ–å‚æ•°
        self.W1 = np.random.randn(hidden_size, input_size) * 0.01
        self.b1 = np.zeros((hidden_size, 1))
        self.W2 = np.random.randn(output_size, hidden_size) * 0.01
        self.b2 = np.zeros((output_size, 1))
    
    def _relu(self, Z):
        return np.maximum(0, Z)
    
    def _relu_derivative(self, Z):
        return Z > 0
    
    def forward(self, X):
        # å‰å‘ä¼ æ’­
        self.Z1 = np.dot(self.W1, X) + self.b1
        self.A1 = self._relu(self.Z1)
        self.Z2 = np.dot(self.W2, self.A1) + self.b2
        self.A2 = self.Z2  # çº¿æ€§è¾“å‡ºï¼ˆå›å½’é—®é¢˜ï¼‰
        
        return self.A2
    
    def compute_loss(self, y_pred, y_true):
        # è®¡ç®—MSEæŸå¤±
        m = y_true.shape[1]
        loss = (1/(2*m)) * np.sum(np.square(y_pred - y_true))
        return loss
    
    def backward(self, X, Y):
        # åå‘ä¼ æ’­
        m = X.shape[1]
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        dZ2 = self.A2 - Y
        dW2 = (1/m) * np.dot(dZ2, self.A1.T)
        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)
        
        # éšè—å±‚æ¢¯åº¦
        dZ1 = np.dot(self.W2.T, dZ2) * self._relu_derivative(self.Z1)
        dW1 = (1/m) * np.dot(dZ1, X.T)
        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)
        
        return dW1, db1, dW2, db2
    
    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):
        # æ›´æ–°å‚æ•°
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
    
    def train(self, X, Y, learning_rate=0.01, epochs=1000):
        losses = []
        
        for i in range(epochs):
            # å‰å‘ä¼ æ’­
            y_pred = self.forward(X)
            
            # è®¡ç®—æŸå¤±
            loss = self.compute_loss(y_pred, Y)
            losses.append(loss)
            
            # åå‘ä¼ æ’­
            dW1, db1, dW2, db2 = self.backward(X, Y)
            
            # æ›´æ–°å‚æ•°
            self.update_parameters(dW1, db1, dW2, db2, learning_rate)
            
            # æ‰“å°æŸå¤±
            if i % 100 == 0:
                print(f"Epoch {i}, Loss: {loss:.6f}")
        
        return losses

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    X = np.random.randn(2, 100)  # 2ä¸ªç‰¹å¾ï¼Œ100ä¸ªæ ·æœ¬
    Y = np.sin(X[0, :]) + np.cos(X[1, :])  # ç›®æ ‡ï¼šä¸€ä¸ªç®€å•çš„éçº¿æ€§å‡½æ•°
    Y = Y.reshape(1, -1)
    
    # åˆ›å»ºå¹¶è®­ç»ƒç½‘ç»œ
    nn = SimpleNN(input_size=2, hidden_size=10, output_size=1)
    losses = nn.train(X, Y, learning_rate=0.1, epochs=1000)
    
    # è¯„ä¼°
    y_pred = nn.forward(X)
    mse = np.mean(np.square(y_pred - Y))
    print(f"Final MSE: {mse:.6f}")
```

## ğŸ“˜ ç›¸å…³èµ„æº

- [è¿”å›æ·±åº¦å­¦ä¹ é¦–é¡µ](./README.md)
- [å·ç§¯ç¥ç»ç½‘ç»œ](./CNN.md)
- [å¾ªç¯ç¥ç»ç½‘ç»œ](./RNN.md)
- [ä¼˜åŒ–ç®—æ³•](./Optimization.md)

---

Â© AIæŠ€æœ¯çŸ¥è¯†åº“ 2023 